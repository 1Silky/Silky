# Install the Google API client library for Python using the following command:
! pip install google-api-python-client
# Install openpyxl using the following commands
! pip install google-api-python-client openpyxl
! pip install python-dateutil
# For gender identification
! git clone https://github.com/Rai220/MlSexDetector.git
! pip install gender-guesser
! pip install pymorphy2
# For sentiment analysis
! pip install transformers
! pip install xlsxwriter
! pip install matplotlib
! pip install pandas

from googleapiclient.discovery import build
api_key = "YOUR API KEY"
youtube = build('youtube', 'v3', developerKey=api_key)

# Import the necessary libraries and set up the credentials by providing your API key:
from googleapiclient.errors import HttpError
import openpyxl
from dateutil import parser
from datetime import datetime, timedelta
import re
from keras import backend as K
from keras.models import model_from_json
from bs4 import BeautifulSoup

  # Gender identification
import os
import gender_guesser.detector as gender
import requests
import pymorphy2

  # Data analysis
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

  # Data visualization 
import plotly.graph_objects as go
import seaborn as sns

# Sentiment analysis
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch
import transformers
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
import pandas as pd


def get_comments(video_ids):
    comments = []

    for video_id in video_ids:
        # Use the videos().list method to retrieve information about the video, including the videoId:
        video_response = youtube.videos().list(part='snippet,contentDetails', id=video_id).execute()
        duration = video_response['items'][0]['contentDetails']['duration']
        video_title = video_response['items'][0]['snippet']['title']
        

        # Use the commentThreads().list method to retrieve the comments for the video:
        nextPageToken = None

        while True:
            response = youtube.commentThreads().list(
                part='snippet',
                videoId=video_id,
                pageToken=nextPageToken,
                maxResults=100
            ).execute()

            for item in response['items']:
                snippet = item.get('snippet')
                if snippet is None:
                    continue

                topLevelComment = snippet.get('topLevelComment')
                if topLevelComment is None:
                    continue

                comment = topLevelComment.get('snippet')
                if comment is None:
                    continue

                comment_html = comment.get('textDisplay')
                if comment_html is None:
                  continue
                
                soup = BeautifulSoup(comment_html, 'html.parser')
                comment_text = soup.get_text()

                comment_date = parser.parse(comment.get('publishedAt')).strftime('%d-%m-%Y')
                commenter_name = comment.get('authorDisplayName')
                comment_time = parser.parse(comment.get('publishedAt')).strftime('%H:%M:%S')
                
                published_at = video_response['items'][0]['snippet']['publishedAt']
                video_publication = parser.parse(published_at).strftime('%d-%m-%Y')

                match = re.search(r'PT(\d+H)?(\d+M)?(\d+S)?', duration)
                hours = int(match.group(1)[:-1]) if match.group(1) else 0
                minutes = int(match.group(2)[:-1]) if match.group(2) else 0
                seconds = int(match.group(3)[:-1]) if match.group(3) else 0
                comment_seconds = (hours * 3600) + (minutes * 60) + seconds

                # Append the comment to the comments list:
                comments.append((comment_date, comment_time, comment_text, comment_seconds, video_id, commenter_name, video_publication))

                # Retrieve the replies to the comment:
                if snippet.get('totalReplyCount', 0) > 0:
                    reply_threads = youtube.comments().list(
                        part='snippet',
                        parentId=topLevelComment.get('id')
                    ).execute()

                    for reply in reply_threads['items']:
                        reply_snippet = reply.get('snippet')
                        if reply_snippet is None:
                            continue

                        reply_text = reply_snippet.get('textDisplay')
                        if reply_text is None:
                            continue

                        reply_date = parser.parse(reply_snippet.get('publishedAt')).strftime('%d-%m-%Y')
                        reply_commenter_name = reply_snippet.get('authorDisplayName')
                        reply_time = parser.parse(reply_snippet.get('publishedAt')).strftime('%H:%M:%S')

                        # Append the reply to the comments list:
                        comments.append((reply_date, reply_time, reply_text, comment_seconds, video_id, reply_commenter_name, video_publication))

            if 'nextPageToken' in response:
                nextPageToken = response['nextPageToken']
            else:
                break

    # Save the Excel file:
    def write_to_excel(comments):
        # Create a new workbook and select the active worksheet:
        workbook = openpyxl.Workbook()
        worksheet = workbook.active

        # Write the comments to the worksheet:
        worksheet.cell(row = 1, column = 1, value = 'Publication Date')
        worksheet.cell(row = 1, column = 2, value = 'Comment time')
        worksheet.cell(row = 1, column = 3, value = 'Comment')
        worksheet.cell(row = 1, column = 4, value = 'Duration of the video')
        worksheet.cell(row = 1, column = 5, value = 'Video ID')
        worksheet.cell(row = 1, column = 6, value = 'Commenter name')
        worksheet.cell(row = 1, column = 7, value = 'Video publication date')

        for i, comment in enumerate(comments):
            worksheet.cell(row=i+2, column=1, value=comment[0])
            worksheet.cell(row=i+2, column=2, value=comment[1])
            worksheet.cell(row=i+2, column=3, value=comment[2])
            worksheet.cell(row=i+2, column=4, value=comment[3])
            worksheet.cell(row=i+2, column=5, value=comment[4])
            worksheet.cell(row=i+2, column=6, value=comment[5])
            worksheet.cell(row=i+2, column=7, value=comment[6])
            

        # Save the workbook:
        filename = r'Comments from videos.xlsx'
        workbook.save(filename)

        print(f'Successfully saved {len(comments)} comments to {filename}.')

    write_to_excel(comments)


# Creating a fancy function to read several comments
def read():
    video_ids = []
    while True:
      line = input()
      video_ids.append(line)
      if line == '':
        video_ids.pop()
        return video_ids
        

# Call the function with a list of video IDs:
video_ids = read()
get_comments(video_ids)
